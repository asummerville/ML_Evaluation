---
title: "Machine Learning Evaluation - Bank and Commercial Classification"
author: "Aatmika Deshpande, Nick Kalinowski, Alden Summerville"
date: "10/25/2020"
output:
  html_document:
    toc: TRUE
    theme: spacelab
    toc_float: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, warning = FALSE, error = FALSE, message = FALSE)

library(DT)
library(tidyverse)
library(ggplot2)
library(class)
library(caret)
library(kableExtra)
library(ROCR)
library(Metrics)
library(MLmetrics)

setwd("/cloud/project/eval")
getwd()

```

## **Objective**

Throughout your early career as a data scientist whether that was exploring NBA talent, guiding climate change policy investment or better understanding how to create better commercials you've suddenly realized you need to enhance your ability to assess the models you are building. As the most important part about understanding any machine learning model is understanding it's weakness or better said it's vulnerabilities. 

In doing so you've decided to revisit your last consulting gigs and gather a sense of how to discuss the good and the bad of these outcomes. 

## **Loan Classification Analysis**

The goal of this kNN model is to predict whether a potential customer will subscribe to their service. When thinking about what metrics might be useful for this case, I believe sensitivity (true positive rate) and log loss would provide the most valuable information, as the client will want a model that is great at classifying prospective customers (true positive rate) and they wouldn't want a wrong classification with high confidence (log loss) because that could translate to a loss in revenue.

### Building the Model

```{r, include=FALSE}
bank_data = read.csv("bank.csv", #<- name of the data set.
                     check.names = FALSE, #<- don't change column names.
                     stringsAsFactors = FALSE)#<- don't convert the numbers and characters to factors.

# Check the structure and view the data.
#str(bank_data)
#view(bank_data)
#Scale the features we will be using for classification 
bank_data[, c("age","duration","balance")] <- lapply(bank_data[, c("age","duration","balance")],function(x) scale(x))

#str(bank_data)
#view(bank_data)
```

Calculating base rate of classifying a customer: 

```{r, echo=FALSE}
# Let's run the kNN algorithm on our banking data. 
# Check the composition of labels in the data set. 
base.rate <- matrix(table(bank_data$`signed up`), ncol=2, nrow=1)
colnames(base.rate) <- c('Incorrect', 'Correct')
rownames(base.rate) <- 'Frequency'
datatable(base.rate)

rate <- matrix(table(bank_data$`signed up`)[2] / sum(table(bank_data$`signed up`)), ncol=1, nrow=1)
colnames(rate) <- 'Base Rate'
rownames(rate) <- ' '
datatable(rate)

# This means that at random, we have an 11.6% chance of correctly picking
# out a subscribed individual. Let's see if kNN can do any better.

# Let's split the data into a training and a test set.
# Sample 80% of our know data as training and 20% as test.
set.seed(1982)
bank_data_train_rows = sample(1:nrow(bank_data),#<- from 1 to the number of 
                                                     #rows in the data set
                              round(0.8 * nrow(bank_data), 0),  #<- multiply the number of rows by 0.8 and round the decimals
                              replace = FALSE)#<- don't replace the numbers

#head(bank_data_train_rows)

# Let's check to make sure we have 80% of the rows. 
#length(bank_data_train_rows) / nrow(bank_data)

bank_data_train = bank_data[bank_data_train_rows, ] #<- select the rows identified in the bank_data_train_rows data
                                                    
bank_data_test = bank_data[-bank_data_train_rows, ]  #<- select the rows that weren't identified in the bank_data_train_rows data

# Check the number of rows in each set.
#nrow(bank_data_train)
#nrow(bank_data_test)

```

This means that at random, we have an 11.6% chance of correctly picking out a subscribed individual.

```{r, include=FALSE}
# Let's train the classifier for k = 3. 
# Install the "class" package that we'll use to run kNN.
# Take some time to learn about all its functionality.


# k-Nearest Neighbor is a randomized algorithm, so make sure to
# use set.seed() to make your results repeatable.
set.seed(1982)
bank_3NN <-  knn(train = bank_data_train[, c("age", "balance", "duration")],#<- training set cases
               test = bank_data_test[, c("age", "balance", "duration")],    #<- test set cases
               cl = bank_data_train[, "signed up"],#<- category for true classification
               k = 3,#<- number of neighbors considered
               use.all = TRUE,
               prob = TRUE) #<- control ties between class assignments If true, all distances equal to the kth largest are included

# View the output.
str(bank_3NN)
length(bank_3NN)
table(bank_3NN)
```

```{r,include=FALSE}
# How does the kNN classification compare to the true class?
# Let's take a look at the confusion matrix by combining the 
# predictions from bank_3NN to the original data set.
kNN_res = table(bank_3NN,
                bank_data_test$`signed up`)
kNN_res
sum(kNN_res)  #<- the total is all the test examples

# Select the true positives and true negatives by selecting
# only the cells where the row and column names are the same.
kNN_res[row(kNN_res) == col(kNN_res)]

# Calculate the accuracy rate by dividing the correct classifications
# by the total number of classifications.
kNN_acc = sum(kNN_res[row(kNN_res) == col(kNN_res)]) / sum(kNN_res)

kNN_acc

# An 87.0% accuracy rate is pretty good but keep in mind the base-rate is roughly 89/11, so we have more or less a 90% chance of guessing right if we don't know anything about the customer, but the negative outcomes we don't really care about, this models value is being able to id sign ups when they are actually sign ups. This requires us to know are true positive rate, or Sensitivity or Recall. (Ya, that's annoying.) So let's dig a little deeper.    

table(bank_data_test$`signed up`)

confusionMatrix(as.factor(bank_3NN), as.factor(bank_data_test$`signed up`), positive = "1", dnn=c("Prediction", "Actual"), mode = "sens_spec")

#So our ability to "predict" sign up customers has more than doubled to 27% so that's  good but still pretty bad overall. This means that out of 10 sign ups, we really only classify 3 correctly. This is fairly typical when we have a unbalanced dataset. Which is why in this case we would want to tune this model on TPR (Sensitivity), to get it has high as possible while sacrificing Specificity or Precision.  Similar to a medical diagnosis example, where we would rather produce false positives as compared to false negatives, predict more of those with cancer that don't have it as compared to missing anyone that actually has cancer.      

#Reference for confusion matrix: https://www.rdocumentation.org/packages/caret/versions/6.0-86/topics/confusionMatrix 

```

After building the model using kNN and using a function to assess the optimal k number of neighbors, this elbow plot is created:

```{r, echo=FALSE}
# How does "k" affect classification accuracy? Let's create a function
# to calculate classification accuracy based on the number of "k."
chooseK = function(k, train_set, val_set, train_class, val_class){
  
  # Build knn with k neighbors considered.
  set.seed(1)
  class_knn = knn(train = train_set,    #<- training set cases
                  test = val_set,       #<- test set cases
                  cl = train_class,     #<- category for classification
                  k = k,                #<- number of neighbors considered
                  use.all = TRUE)       #<- control ties between class assignments
                                        #   If true, all distances equal to the kth largest are included
  conf_mat = table(class_knn, val_class)
  
  # Calculate the accuracy.
  accu = sum(conf_mat[row(conf_mat) == col(conf_mat)]) / sum(conf_mat)                         
  cbind(k = k, accuracy = accu)
}



# The sapply() function plugs in several values into our chooseK function.
#sapply(x, fun...) "fun" here is passing a function to our k-function
# function(x)[function] allows you to apply a series of numbers
# to a function without running a for() loop! Returns a matrix.
knn_different_k = sapply(seq(1, 21, by = 2),  #<- set k to be odd number from 1 to 21
                         function(x) chooseK(x, 
                                             train_set = bank_data_train[, c("age", "balance", "duration")],
                                             val_set = bank_data_test[, c("age", "balance", "duration")],
                                             train_class = bank_data_train[, "signed up"],
                                             val_class = bank_data_test[, "signed up"]))



#A bit more of a explanation...
#seq(1,21, by=2)#just creates a series of numbers
#sapply(seq(1, 21, by=2), function(x) x+1)#sapply returns a new vector using the series of numbers and some calculation that is repeated over the vector of numbers 


# Reformatting the results to graph
#str(knn_different_k)
#class(knn_different_k)#matrix 
#head(knn_different_k)

knn_different_k = data.frame(k = knn_different_k[1,],
                             accuracy = knn_different_k[2,])

# Plot accuracy vs. k.

ggplot(knn_different_k,
       aes(x = k, y = accuracy)) +
  geom_line(color = "green", size = 1.5) +
  geom_point(size = 3)+
  ggtitle("Customer Classification kNN Accuracy")

# 5 to 7 nearest neighbors seems to be a good choice because that's the
# greatest improvement in predictive accuracy before the incremental 
# improvement trails off.

```

The marginal improvement of the accuracy drops at a k value of 5-7, so we'll run our model with k=5 nearest neighbors. The confusion matrix for k=5 is below:

```{r, echo=FALSE}
#retraining the model with k=5

set.seed(1982)
bank_5NN <-  knn(train = bank_data_train[, c("age", "balance", "duration")],#<- training set cases
               test = bank_data_test[, c("age", "balance", "duration")],    #<- test set cases
               cl = bank_data_train[, "signed up"],#<- category for true classification
               k = 5,#<- number of neighbors considered
               use.all = TRUE,
               prob = TRUE)

confusionMatrix(as.factor(bank_5NN), as.factor(bank_data_test$`signed up`), positive = "1", dnn=c("Prediction", "Actual"), mode = "sens_spec")

#extracting probabilities

prob_knn.bank <- tibble(bank_5NN, attributes(bank_5NN)$prob)
prob_knn.bank$prob <- if_else(prob_knn.bank$bank_5NN==0,1-prob_knn.bank$`attributes(bank_5NN)$prob`, prob_knn.bank$`attributes(bank_5NN)$prob`)

```

### Metrics Analysis

- **Accuracy** = 87.52%

- **Sensitivity (true positive rate)** = 23.79%

- **False positive rate** = 3.91%

- **F1 Score** = 0.31

- **Kappa** = 0.2497

- **AUC** = 0.728

- **Bias** = -0.944

- **Log Loss** = 1.505

From the confusion matrix above, the accuracy of our model with k=5 is 87.52%. This is great for the accuracy statistic, however it may give a biased representation to correctly classifying positive cases or negative cases, as it takes both into account. To dig deeper, we'll look at the true positive rate (or sensitivity) and false positive rate (1-specificity). 

The sensitivity is 23.79% which is very poor. On the other hand, the false positive rate is 3.91% which is excellent. Those metrics basically tell us the model is terrible at correctly classifying a customer who will sign up, but is excellent at classifying when a customer won't. Therefore, applying this model in real-life might not be a good idea if the bank wants to find new customers; however, if they for some reason want to classify who *isn't* a good fit for a customer, this model is excellent at that.

```{r, echo=FALSE}

#f1 = 2*(precision*recall/(precision+recall)) , recall=sensitivity
prec = 0.4497257
recall = 0.23791

f1 <- 2*(prec*recall/(prec+recall))
#score of 0.31 ... not very good

#kappa = 0.2497

```

The F1 score (a measure of accuracy that is the harmonic mean of precision and recall) is 0.31 which is poor. The F1 score takes into account the precision predicting positive outcomes, and the proportion of actual positive correct outcomes, therefore, because the F1 is poor we have more confirmation that our model is deficient at classifying positive outcomes (a prospective customer). 

Another metric, Kappa, which is indicates how much better our classifier is performing over the performance of a classifier that would just guess at random, is equal to 0.2497. That is also poor, indicating our model is not much better than simply guessing at random (for classiying positive cases). 

```{r, echo=FALSE}

#ROC and AUC

#putting probs and outcomes in same df
bank_data_test <- data.frame(pred_class=bank_data_test, pred_prob=prob_knn.bank$`prob`,target=as.numeric(bank_data_test$`signed up`))
#view(bank_data_test)

pred.bank <- prediction(bank_data_test$pred_prob,bank_data_test$target)

tree_perf.bank <- performance(pred.bank,"tpr","fpr")

plot(tree_perf.bank, colorize=TRUE)
#abline(a=0, b=1)

tree_perf_AUC.bank <- performance(pred.bank,"auc")

#print(tree_perf_AUC.bank@y.values) #value of 0.728

```

Another useful metric uses an ROC (receiver operating curve) which plots the sensitivity versus specificity at varying cutoff thresholds (the probabilistic threshold the model uses to classify a case as positive). An AUC (area under curve) value is calculated based on the ROC, and the value for this model is 0.728 which is a fair rating as we want the value to be >0.8.


```{r, echo = FALSE}

#Bias
view(bank_data_test)
bias.bank <- bias(as.numeric(bank_data_test$target), as.numeric(bank_5NN))
#-0.944 ... poor reading

#log loss

ll.bank <- LogLoss(as.numeric(bank_data_test$pred_prob), as.numeric(bank_data_test$target))
#1.505

#-log(ll.bank)
#-0.409

```

The bias metric is -0.944 which is poor, as an unbiased model would have a reading of zero. The model bias tends to classify an outcome as a non-customer, which is consistent with the very low false positive rate. 

Log Loss is another useful metric that measures the uncertainty of the probabilities associated with the model and compares them to the actual classifications. This is beneficial because it heavily penalizes instances of high confidence (high probability) in classifying a value incorrectly. The value for this model is 1.505, which is poor as we want a number close to zero. This means there is much uncertainty in our model's probabilities for classification.

After assessing the above metrics, because the goal of the model is to predict potential customers (the positive case), the metrics that we care about the most include:

- Sensitivity (high)

- F1 Score (high)

- False Positive Rate (low)

To try and reach the goals above, we can change the threshold at which the model will proceed with a classification (typically is 0.5). By referencing the ROC curve to gauge the performance of other thresholds, if we lower the threshold to around 0.1, the model performs much more balanced than using the previous threshold of 0.5. A new confusion matrix is produced to reflect the new model:

```{r, echo=FALSE}

#change thresholds

adjust_thres <- function(x, y, z) {
  #x=pred_probablities, y=threshold, z=test_outcome
  thres <- as.factor(ifelse(x > y, 1,0))
  confusionMatrix(thres, z, positive = "1", dnn=c("Prediction", "Actual"), mode = "everything")
}

adjust_thres(bank_data_test$pred_prob,.10, as.factor(bank_data_test$target))

```
Although the accuracy drops to ~70%, the sensitivity (previously ~24%) increases to ~70% and the specificity (previously ~96%) drops to ~70% which isn't too bad. The primary advantage to lowering the threshold is increasing the true positive rate, meaning more correct classifications for potential customers.

### Summary of Findings

Overall, based on the above metrics, it is evident that our model is deficient at predicting customers who are likely to sign up, but is excellent at classifying customers who won't. This is primarily due to an **unbalanced dataset** that contains many more negative cases (customers who didn't sign up) than positive cases (customers who did sign up); in the dataset, only ~5,000/43,000 signed up, so the model training was very unbalanced, leading to a poor rate of classifying prospective customers. If we lower the cutoff threshold of the model to 0.1, we can increase the true positive rate to ~70%, with the sacrifice of a higher false positive rate.

I would not recommend the bank to use this model (unless they only want to classify customers who will not sign up), but if they obtain a balanced dataset with more "signed up" cases, the model could be re-trained and would likely increase its performance for classifying customers who will sign up for their program. If they want to proceed with the model, I would recommend changing the threshold to 0.1, however, the model still is not an excellent classifier for finding potential customers due to an average true positive rate and ~30% false positive rate.

## **Commercial Classification Analysis**

The goal of this kNN model is to correctly classify if a TV "snippet" is a commercial or a non-commercial. 

### Building the Model

```{r, include=FALSE}
#1
#Load in the data, both the commercial dataset and the labels. You'll need to the place the labels on the columns. The dataset "tv_commercialsets-CNN_Cleaned.csv",  is data collected about the features of commercials on CNN. We can try to predict what segments of video are commercials based on their audio and video components. More information on the datasets can be found data.world:
# https://data.world/kramea/tv-commercial-detection/workspace/file?filename=tv_commercial_datasets%2FBBC_Cleaned.csv

#You can use the function colnames() to apply the labels (hint: you might need to reshape the labels to make this work)

CNN_commercials <- read.csv("tv_commercial_datasets_CNN_Cleaned.csv")
CNN_labels = read.csv("cnn_commmercial_label.csv", header=FALSE)
colnames(CNN_commercials) = CNN_labels[,1]


```

Calculating base rate of classifying a commercial: 

```{r, echo=FALSE}
#2. Determine the split between commercial and non-commercial then calculate the base rate, assume 1 is the commercial label and -1 is the non-commercial label 

base.mat2 <- matrix(table(CNN_commercials$`label`), ncol=2, nrow=1)
colnames(base.mat2) <- c('Non-Commercial', 'Commercial')
rownames(base.mat2) <- 'Frequency'
datatable(base.mat2)

rate2 <- matrix(table(CNN_commercials$`label`)[2] / sum(table(CNN_commercials$`label`)), ncol=1, nrow = 1)
colnames(rate2) <- 'Rate'
rownames(rate2) <- ' '
datatable(rate2)

#The calculated baseline rate of commercials to non-commercials is 63.92%, given no information.

```

This means that at random, we have an 63.92% chance of correctly classifying a commercial.

```{r, include=FALSE}
#3. Since there are columns that contain different metrics for the same variable (i.e. any column that ends in 'mn' is the mean of that variable, while any column that ends in 'var' is the variance of that variable), we don't need to keep both, drop all the columns that include var

CNN_commercials <- select(CNN_commercials, -ends_with("var"))

```

```{r, echo=FALSE}
#4.  Before we run knn, sometimes it's good to check to make sure that our variables are not highly correlated. Use the cor() function on 'your_dataframe', label it 'commercial_correlations', and view the data.

commercial_correlations <- cor(CNN_commercials)

```

```{r, include=FALSE}
#5. Determine which variables to remove, high correlations start around .7 or below -.7 I would especially remove variables that appear to be correlated with more than one variable. List your rationale here:

corr_means <- matrix(c(mean(abs(as.data.frame(commercial_correlations)$motion_distr_mn)), mean(abs(as.data.frame(commercial_correlations)$motion_dist_mn)), mean(abs(as.data.frame(commercial_correlations)$spectral_centroid_mn)), mean(abs(as.data.frame(commercial_correlations)$spectral_roll_off_mn)), mean(abs(as.data.frame(commercial_correlations)$spectral_flux_mn))), ncol=1)
colnames(corr_means) <- 'Mean Correlation'
rownames(corr_means) <- c('motion_distr_mn', 'motion_dist_mn', 'spectral_centroid_mn', 'spectral_roll_off_mn', 'spectral_flux_mn')
#mean_table <- as.table(corr_means)
datatable(corr_means)

```

```{r, include=FALSE}
#6. Subset the dataframe based on above.

CNN_commercials <- select(CNN_commercials, -motion_distr_mn, -spectral_centroid_mn, -spectral_flux_mn)

```

```{r, include=FALSE}
#7. Now we have our data and are ready to run the KNN, but we need to split into test and train. Create a index the will divide the data into a 70/30 split

CNN_index <- round(0.7 * nrow(CNN_commercials), 0)

```

```{r, include=FALSE}
#8. Use the index above to generate a train and test sets, then check the row counts to be safe and show Mr. Rooney. 

set.seed(10271999)

CNN_train_rows <- sample(1:nrow(CNN_commercials), 
                         CNN_index, 
                         replace = FALSE)

#generate train and test sets
CNN_train <- CNN_commercials[CNN_train_rows, ]
CNN_test <- CNN_commercials[-CNN_train_rows, ]

#check train set
accuracy <- nrow(CNN_train)/(nrow(CNN_train)+nrow(CNN_test))
numrow.train <- nrow(CNN_train)
numrow.test <- nrow(CNN_test)

split_matrix <- matrix(c(numrow.train, numrow.test, accuracy, 1 - accuracy), ncol=4, nrow=1)
colnames(split_matrix) <- c('# Rows in Train Set', '# Rows in Test Set', 'Ratio of Train to Test', 'Ratio of Test to Train')
rownames(split_matrix) <- 'Split'
datatable(split_matrix)

```

```{r, include=FALSE}
#9 Train the classifier using k = 3, remember to set.seed so you can repeat the output and to use the labels as a vector for the class (not a index of the dataframe)

CNN_KNN <- knn(train = CNN_train[, 1:7],
               test = CNN_test[, 1:7],  
               cl = CNN_train$`label `,
               k = 3,
               use.all = TRUE,
               prob = TRUE)

```


```{r, include=FALSE}
#10 Check the output using str and length just to be sure it worked

str(CNN_KNN)
length(CNN_KNN)

```

```{r, include=FALSE}
#11 Create a initial confusion matrix using the table function and pass it to a object. (xx <- your confusion matrix)

CNN_conf_matrix <- table(CNN_KNN, CNN_test$`label `)
#conf.matrix <- as.matrix(CNN_conf_matrix)
#datatable(conf.matrix)

conf.mat <- matrix(CNN_conf_matrix)
con.mat <- matrix(1:6, ncol=3, nrow=2)
colnames(con.mat) <- c('CNN_KNN', '-1', '1')
con.mat[1,1] <- -1
con.mat[2,1] <- 1
con.mat[1,2] <- 1314
con.mat[2,2] <- 1072
con.mat[1,3] <- 743
con.mat[2,3] <- 3635
datatable(con.mat)
```

```{r, include=FALSE}
#12 Select the true positives and true negatives by selecting only the cells where the row and column names are the same.

tp.tn.table <- CNN_conf_matrix[row(CNN_conf_matrix) == col(CNN_conf_matrix)]

tp.tn.mat <- matrix(c(tp.tn.table[1], tp.tn.table[2]), nrow=1, ncol=2)
colnames(tp.tn.mat) <- c('True Negatives', 'True Positives')
rownames(tp.tn.mat) <- 'Frequency'
datatable(tp.tn.mat)


```

```{r, include=FALSE}
#13 Calculate the accuracy rate by dividing the correct classifications by the total number of classifications. Label the data 'kNN_acc_com', and view it. Comment on how this compares to the base rate. 

kNN_acc_com <- sum(CNN_conf_matrix[row(CNN_conf_matrix) == col(CNN_conf_matrix)])/sum(CNN_conf_matrix)
kNN_acc_com

#The accuracy rate calculates to 73.17%. Compared to the base accuracy rate of 63.92%, there was an increase in about 10% accuracy using our machine learning model.

```

```{r, include=FALSE}
#14  Run the confusion matrix function and comment on the model output

confusionMatrix(as.factor(CNN_KNN), as.factor(CNN_test$`label `), positive = "1", dnn=c("Prediction", "Actual"), mode = "sens_spec")

```

```{r, include=FALSE}
#15 Run the "chooseK" function to find the perfect K, while using sapply() function on chooseK() to test k from 1 to 21 (only selecting the odd numbers), and set the train_set argument to 'commercial_train', val_set to 'commercial_test', train_class to the "label"   column of 'commercial_train', and val_class to the "label" column of 'commercial_test'. Label this  "knn_diff_k_com"

chooseK = function(k, train_set, val_set, train_class, val_class){
  
  # Build knn with k neighbors considered.
  set.seed(1)
  class_knn = knn(train = train_set,    #<- training set cases
                  test = val_set,       #<- test set cases
                  cl = train_class,     #<- category for classification
                  k = k,                #<- number of neighbors considered
                  use.all = TRUE)       #<- control ties between class assignments
                                        #   If true, all distances equal to the kth largest are included
  conf_mat = table(class_knn, val_class)
  
  # Calculate the accuracy.
  accu = sum(conf_mat[row(conf_mat) == col(conf_mat)]) / sum(conf_mat)                         
  cbind(k = k, accuracy = accu)
}


knn_diff_k_com = sapply(seq(1, 21, by = 2),
                         function(x) chooseK(x, 
                                             train_set = CNN_train[, 1:7],
                                             val_set = CNN_test[, 1:7],
                                             train_class = CNN_train$`label `,
                                             val_class = CNN_test$`label `))


```

```{r, include=FALSE}
#16 Create a dataframe so we can visualize the difference in accuracy based on K, convert the matrix to a dataframe

k_output = data.frame(k = knn_diff_k_com[1,],
                             accuracy = knn_diff_k_com[2,])
k.output.mat <- as.matrix(k_output)
datatable(k.output.mat)

```

After building the model using kNN and using a function to assess the optimal k number of neighbors, this elbow plot is created:

```{r, echo=FALSE}
#17 Use ggplot to show the output and comment on the k to select

ggplot(k_output,
       aes(x = k, y = accuracy)) +
  geom_line(color = "green", size = 1) +
  geom_point(size = 2)+
  ggtitle('Commercial Classification kNN Accuracy')


```

Although the accuracy marginally loses improvement at k=5, the peak of the curve that gives us the highest accuracy is at k=11; therefore, the optimal k option for our model is k = 11.

```{r, include=FALSE}
#18 Rerun the model  with "optimal" k 

CNN_KNN_optimal <- knn(train = CNN_train[, 1:7],
               test = CNN_test[, 1:7],  
               cl = CNN_train$`label `,
               k = 11,
               use.all = TRUE,
               prob = TRUE)


prob_knn.comm <- tibble(CNN_KNN_optimal, attributes(CNN_KNN_optimal)$prob)
prob_knn.comm <- as.data.frame(prob_knn.comm)
prob_knn.comm$prob <- if_else(prob_knn.comm$CNN_KNN_optimal==-1,1-prob_knn.comm$`attributes(CNN_KNN_optimal)$prob`, prob_knn.comm$`attributes(CNN_KNN_optimal)$prob`)
#view(prob_knn.comm)

```

The confusion matrix for k=11 is below:

```{r, echo=FALSE}
#19 Use the confusion matrix function to measure the quality of the new model

confusionMatrix(as.factor(CNN_KNN_optimal), as.factor(CNN_test$`label `), positive = "1", dnn=c("Prediction", "Actual"), mode = "sens_spec")

#Accuracy = 76.35%
#Sensitivity = 88.65%

#The new accuracy is 76.35%, marking an increase of about 3% from the original k=3 model. However, a more important metric, the true positive rate or sensitivity, is 88.65% which is excellent. The positive and negative prediction rates are also fairly similar, differing by ~5%, meaning there is little bias in the model to predicting positive over negative, or vice versa. Finally, another important metric is the balanced accuracy which is the average of the sensitivity and specificity--for k=11, the balanced accuracy is 71.19% which is a solid (while not excellent) value for our model.

#Mr. Rooney, after building your desired machine learning model, I was able to increase the prediction rate of a commercial vs. a non-commercial by around 13%. By utilizing a "k-nearest neighbor" algorithm and adjusting the parameters to include data that would give us optimal results, the model correctly predicted a commercial 76% of the time. The model did this by calculating something called a "euclidean distance" which is basically the distance between a point we wish to label and other known points. By optimizing the number of known points the model searches for to classify an unknown point, the model was able to reach a prediction accuracy of 76% with a sensitivity of 89%. Compared to the rate of 63% for correct predictions with no given information (the baseline rate), my model is valid and could be applied in the field. Another advantage of this model is that it can be re-trained if given new data and can be continuously updated to reflect changes or trends in modern TV commercials. 

```

### Metrics Analysis

- **Accuracy** = 76.35%

- **Sensitivity (true positive rate)** = 88.65%

- **False positive rate** = 46.23%

- **F1 Score** = 0.8291

- **Kappa** = 0.4502

- **AUC** = 0.778

- **Bias** = -1.442

- **Log Loss** = -0.0719

When first looking at the metrics the confusion matrix outputs, the accuracy is fair with a value of 76.35%, and the sensitivity is great with a value of 88.65%. However, the false positive rate, 46.23%, is poor. This tells us the model is excellent at classifying positive outcomes (commercials), but is probably because it tends to classify the outcome a commercial. So far the model is looking okay, but let's dig a little deeper.

```{r, echo=FALSE}

#f1 = 2*(precision*recall/(precision+recall)) , recall=sensitivity
prec.comm = 0.77869
recall.comm = .8865

f1.comm <- 2*(prec*recall/(prec+recall))
#score of 0.8291 ... good

#kappa = 0.4502


```

After calculating the F1 score and Kappa, the model is further validated. The F1 comes out to 0.8291, which again tells us the model is excellent at predicting positive outcomes (commercials). The Kappa statistic is 0.4502, while not great, is an okay measure and tells us our model is fairly better at predicting commercials versus non-commercials than if you were to guess at random. 

```{r, echo=FALSE}

#ROC and AUC

#putting probs and outcomes in same df
CNN_test <- data.frame(pred_class=CNN_test, pred_prob=prob_knn.comm$`prob`,target=as.numeric(CNN_test$`label `))


pred.comm <- prediction(CNN_test$pred_prob,CNN_test$target)

tree_perf.comm <- performance(pred.comm,"tpr","fpr")

plot(tree_perf.comm, colorize=TRUE)
#abline(a=0, b=1)

tree_perf_AUC.comm <- performance(pred.comm,"auc")

#print(tree_perf_AUC.comm@y.values) #value of 0.778


```
The AUC value derived from the ROC curve is 0.778 which is a good measure, but is most likely negatively impacted by a high false positive rate. Based on the metrics so far, it's evident that the model is biased towards classifying a commercial rather than non-commercial, which is likely due to an unbalanced data set with ~14.4k/22.5k outcomes being commercials. Let's look at bias and log loss, then see if we can lower the false positive rate by changing the threshold of the model.


```{r, echo=FALSE}

#Bias

bias.comm <- bias(as.numeric(CNN_test$target), as.numeric(CNN_KNN_optimal))
#-1.442

#log loss

ll.comm <- LogLoss(as.numeric(CNN_test$pred_prob), as.numeric(CNN_test$target))
#-0.0719


```

The bias of the model is -1.442 which is a poor reading and tells us the model is fairly biased with a tendency to classify an outcome as a commercial over a non-commercial (consistent with the high false positive rate). Another useful metric, log loss, tells us if the model has high confidence in predicting classifications in the wrong direction. The value for this model is -0.0719 which is excellent. This means that when the model incorrectly classifies a case, it rarely is incorrect with high confidence in its prediction. 

After assessing the above metrics, because the TV provider wants to classify commercials (the positive case), these metrics are the most important:

- Sensitivity (high)

- False Positive Rate (low)

- F1 Score (high)

Finally, let's adjust the probability threshold at which the model moves forward with a classification. By referencing the ROC curve we can find a threshold that lowers the false positive rate while keeping the true positive rate high. If we adjust the threshold to 0.65 this new confusion matrix is produced:

```{r, echo=FALSE}

#change thresholds

adjust_thres <- function(x, y, z) {
  #x=pred_probablities, y=threshold, z=test_outcome
  thres <- as.factor(ifelse(x > y, 1,-1))
  confusionMatrix(thres, z, positive = "1", dnn=c("Prediction", "Actual"), mode = "everything")
}

adjust_thres(CNN_test$pred_prob,.65, as.factor(CNN_test$target))


```
The sensitivity drops to ~70% but the false positive rate decreases to ~30% which is much better than the previous value of ~47%. The precision and recall remain high (leads to a fair F1 score of 0.75), indicating the model is still great at predicting positive cases, while having a low false positive rate. This is what we want in this situation, as the TV provider wants to classify commercials (the positive case).

### Summary of Findings

Overall, originally the model performed okay, with a high true positive rate but also a high false positive rate. By adjusting the threshold of the model to 0.65, the false positive rate dropped to ~30% and the true positive rate remained high at ~70%. The original error was likely due to an **unbalanced dataset** with ~14.4k/22.5k outcomes being commercials. Therefore, the TV provider could either re-train the model with a more balanced set (more non-commercial cases), or can proceed with the original model but with a changed threshold of 0.65 to minimize the false positive rate while maintaining decent positive classification metrics.


